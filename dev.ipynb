{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data_gen import ExpressionDataset, bert_collate_strs, EXPR_TOKENIZER\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from model import BERTModule\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "fds = ExpressionDataset.from_csv(\"data/equation_dataset.csv\")\n",
    "l = len(fds)\n",
    "a = l *4 //5\n",
    "b = l//5\n",
    "c = l - a - b\n",
    "train_ds, val_ds, test_ds = random_split(fds, [a, b, c], generator=torch.Generator().manual_seed(1))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=256, collate_fn=bert_collate_strs)\n",
    "val_dl = DataLoader(val_ds, shuffle=False, batch_size=512, collate_fn=bert_collate_strs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 22])\n"
     ]
    }
   ],
   "source": [
    "val_b = [val_ds[0], val_ds[1], val_ds[2]]\n",
    "a, b, c, d = next(iter(val_dl))\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 14,  4, 19, 13, 16, 17,  4,  6],\n",
      "        [ 2, 11, 15, 11,  6,  0,  0,  0,  0],\n",
      "        [ 2, 12,  8,  4, 12, 17, 17, 13,  6]])\n",
      "tensor([[False, False,  True, False, False, False, False,  True, False],\n",
      "        [False, False,  True, False, False, False, False, False, False],\n",
      "        [False, False, False,  True, False, False,  True, False, False]]) tensor([ 4,  4, 15,  4, 17])\n",
      "tensor([[ 2, 14, 15, 19, 13, 16, 17, 16,  6],\n",
      "        [ 2, 11, 19, 11,  6,  0,  0,  0,  0],\n",
      "        [ 2, 12,  8, 19, 12, 17, 11, 13,  6]])\n"
     ]
    }
   ],
   "source": [
    "masked_src_id, masked_token_mask, src_id, src_pad_mask = bert_collate_strs(val_b)\n",
    "\n",
    "print(masked_src_id)\n",
    "print(masked_token_mask, masked_src_id[masked_token_mask])\n",
    "print(src_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2, 14, 15, 21, 13, 16, 17, 16,  6],\n",
      "        [ 2, 11, 21, 11,  6,  0,  0,  0,  0],\n",
      "        [ 4, 12,  8, 21, 12, 17, 11, 13,  6]])\n",
      "tensor([[ 2, 14, 15, 21, 13, 16, 17, 16,  6],\n",
      "        [ 2, 11, 21, 11,  6,  0,  0,  0,  0],\n",
      "        [ 2, 12,  8, 21, 12, 17, 11, 13,  6]])\n"
     ]
    }
   ],
   "source": [
    "print(masked_src_id)\n",
    "print(src_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "import vocab\n",
    "print(vocab.MASK_IDX, vocab.CLS_IDX)\n",
    "print(vocab.EXPR_TOKENIZER.get_vocab())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer(vocabulary_size=19, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "WordPiece error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andrew/BERT_example/dev.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B128.32.125.188/home/andrew/BERT_example/dev.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(EXPR_TOKENIZER)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B128.32.125.188/home/andrew/BERT_example/dev.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(EXPR_TOKENIZER\u001b[39m.\u001b[39;49mencode(val_ds[\u001b[39m0\u001b[39;49m]))\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py:216\u001b[0m, in \u001b[0;36mBaseTokenizer.encode\u001b[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=212'>213</a>\u001b[0m \u001b[39mif\u001b[39;00m sequence \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=213'>214</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mencode: `sequence` can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be `None`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=215'>216</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode(sequence, pair, is_pretokenized, add_special_tokens)\n",
      "\u001b[0;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "print(EXPR_TOKENIZER.encode(val_ds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "WordPiece error: Missing [UNK] token from the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/andrew/BERT_example/dev.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B128.32.125.188/home/andrew/BERT_example/dev.ipynb#ch0000003vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(EXPR_TOKENIZER\u001b[39m.\u001b[39;49mencode_batch([val_ds[\u001b[39m0\u001b[39;49m], val_ds[\u001b[39m1\u001b[39;49m], val_ds[\u001b[39m2\u001b[39;49m]]))\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py:253\u001b[0m, in \u001b[0;36mBaseTokenizer.encode_batch\u001b[0;34m(self, inputs, is_pretokenized, add_special_tokens)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=249'>250</a>\u001b[0m \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=250'>251</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mencode_batch: `inputs` can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be `None`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///home/andrew/anaconda3/envs/research/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py?line=252'>253</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(inputs, is_pretokenized, add_special_tokens)\n",
      "\u001b[0;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
     ]
    }
   ],
   "source": [
    "print(EXPR_TOKENIZER.encode_batch([val_ds[0], val_ds[1], val_ds[2]]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "408269ca62060850a8947a0d7a551465652cb7138ef02f7a1c955677fbaf7202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
